{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce91554f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ssury\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing library\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1175e190",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"D:\\BlackCoffer Project\\Input.xlsx\")[['URL_ID','URL']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2b3ba3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Netclan20241017</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-ml-bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Netclan20241018</td>\n",
       "      <td>https://insights.blackcoffer.com/enhancing-fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Netclan20241019</td>\n",
       "      <td>https://insights.blackcoffer.com/roas-dashboar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Netclan20241020</td>\n",
       "      <td>https://insights.blackcoffer.com/efficient-pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netclan20241021</td>\n",
       "      <td>https://insights.blackcoffer.com/development-o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            URL_ID                                                URL\n",
       "0  Netclan20241017  https://insights.blackcoffer.com/ai-and-ml-bas...\n",
       "1  Netclan20241018  https://insights.blackcoffer.com/enhancing-fro...\n",
       "2  Netclan20241019  https://insights.blackcoffer.com/roas-dashboar...\n",
       "3  Netclan20241020  https://insights.blackcoffer.com/efficient-pro...\n",
       "4  Netclan20241021  https://insights.blackcoffer.com/development-o..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db035ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.iloc[0:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7db4f4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Netclan20241017</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-ml-bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Netclan20241018</td>\n",
       "      <td>https://insights.blackcoffer.com/enhancing-fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Netclan20241019</td>\n",
       "      <td>https://insights.blackcoffer.com/roas-dashboar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Netclan20241020</td>\n",
       "      <td>https://insights.blackcoffer.com/efficient-pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netclan20241021</td>\n",
       "      <td>https://insights.blackcoffer.com/development-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Netclan20241159</td>\n",
       "      <td>https://insights.blackcoffer.com/population-an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Netclan20241160</td>\n",
       "      <td>https://insights.blackcoffer.com/google-lsa-ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Netclan20241161</td>\n",
       "      <td>https://insights.blackcoffer.com/healthcare-da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Netclan20241162</td>\n",
       "      <td>https://insights.blackcoffer.com/budget-sales-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Netclan20241163</td>\n",
       "      <td>https://insights.blackcoffer.com/amazon-buy-bo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              URL_ID                                                URL\n",
       "0    Netclan20241017  https://insights.blackcoffer.com/ai-and-ml-bas...\n",
       "1    Netclan20241018  https://insights.blackcoffer.com/enhancing-fro...\n",
       "2    Netclan20241019  https://insights.blackcoffer.com/roas-dashboar...\n",
       "3    Netclan20241020  https://insights.blackcoffer.com/efficient-pro...\n",
       "4    Netclan20241021  https://insights.blackcoffer.com/development-o...\n",
       "..               ...                                                ...\n",
       "142  Netclan20241159  https://insights.blackcoffer.com/population-an...\n",
       "143  Netclan20241160  https://insights.blackcoffer.com/google-lsa-ap...\n",
       "144  Netclan20241161  https://insights.blackcoffer.com/healthcare-da...\n",
       "145  Netclan20241162  https://insights.blackcoffer.com/budget-sales-...\n",
       "146  Netclan20241163  https://insights.blackcoffer.com/amazon-buy-bo...\n",
       "\n",
       "[147 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c43ec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('URL_ID',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58d80a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f4534f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ssury\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:25: DeprecationWarning: Call to deprecated method findAll. (Replaced by find_all) -- Deprecated since version 4.0.0.\n",
      "C:\\Users\\ssury\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:33: DeprecationWarning: Call to deprecated method findAll. (Replaced by find_all) -- Deprecated since version 4.0.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: 1.txt\n",
      "Saved: 2.txt\n",
      "Saved: 3.txt\n",
      "Saved: 4.txt\n",
      "Saved: 5.txt\n",
      "Saved: 6.txt\n",
      "Saved: 7.txt\n",
      "Saved: 8.txt\n",
      "Saved: 9.txt\n",
      "Saved: 10.txt\n",
      "Saved: 11.txt\n",
      "Saved: 12.txt\n",
      "Saved: 13.txt\n",
      "Saved: 14.txt\n",
      "Saved: 15.txt\n",
      "Saved: 16.txt\n",
      "Saved: 17.txt\n",
      "Saved: 18.txt\n",
      "Saved: 19.txt\n",
      "Saved: 20.txt\n",
      "Saved: 21.txt\n",
      "Saved: 22.txt\n",
      "Saved: 23.txt\n",
      "Saved: 24.txt\n",
      "Saved: 25.txt\n",
      "Saved: 26.txt\n",
      "Saved: 27.txt\n",
      "Saved: 28.txt\n",
      "Saved: 29.txt\n",
      "Saved: 30.txt\n",
      "Saved: 31.txt\n",
      "Saved: 32.txt\n",
      "Saved: 33.txt\n",
      "Saved: 34.txt\n",
      "Saved: 35.txt\n",
      "Saved: 36.txt\n",
      "Saved: 37.txt\n",
      "Saved: 38.txt\n",
      "Saved: 39.txt\n",
      "Saved: 40.txt\n",
      "Saved: 41.txt\n",
      "Saved: 42.txt\n",
      "Saved: 43.txt\n",
      "Saved: 44.txt\n",
      "Saved: 45.txt\n",
      "Saved: 46.txt\n",
      "Saved: 47.txt\n",
      "Saved: 48.txt\n",
      "Saved: 49.txt\n",
      "Saved: 50.txt\n",
      "Saved: 51.txt\n",
      "Saved: 52.txt\n",
      "Saved: 53.txt\n",
      "Saved: 54.txt\n",
      "Saved: 55.txt\n",
      "Saved: 56.txt\n",
      "Saved: 57.txt\n",
      "Saved: 58.txt\n",
      "Saved: 59.txt\n",
      "Saved: 60.txt\n",
      "Saved: 61.txt\n",
      "Saved: 62.txt\n",
      "Saved: 63.txt\n",
      "Saved: 64.txt\n",
      "Saved: 65.txt\n",
      "Saved: 66.txt\n",
      "Saved: 67.txt\n",
      "Saved: 68.txt\n",
      "Saved: 69.txt\n",
      "Saved: 70.txt\n",
      "Saved: 71.txt\n",
      "Saved: 72.txt\n",
      "Saved: 73.txt\n",
      "Saved: 74.txt\n",
      "Saved: 75.txt\n",
      "Saved: 76.txt\n",
      "Saved: 77.txt\n",
      "Saved: 78.txt\n",
      "Saved: 79.txt\n",
      "Saved: 80.txt\n",
      "Saved: 81.txt\n",
      "Saved: 82.txt\n",
      "Saved: 83.txt\n",
      "Saved: 84.txt\n",
      "Saved: 85.txt\n",
      "Saved: 86.txt\n",
      "Saved: 87.txt\n",
      "Saved: 88.txt\n",
      "Saved: 89.txt\n",
      "Saved: 90.txt\n",
      "Saved: 91.txt\n",
      "Saved: 92.txt\n",
      "Saved: 93.txt\n",
      "Saved: 94.txt\n",
      "Saved: 95.txt\n",
      "Saved: 96.txt\n",
      "Saved: 97.txt\n",
      "Saved: 98.txt\n",
      "Saved: 99.txt\n",
      "Saved: 100.txt\n",
      "Saved: 101.txt\n",
      "Saved: 102.txt\n",
      "Saved: 103.txt\n",
      "Saved: 104.txt\n",
      "Saved: 105.txt\n",
      "Saved: 106.txt\n",
      "Saved: 107.txt\n",
      "Saved: 108.txt\n",
      "Saved: 109.txt\n",
      "Saved: 110.txt\n",
      "Saved: 111.txt\n",
      "Saved: 112.txt\n",
      "Saved: 113.txt\n",
      "Saved: 114.txt\n",
      "Saved: 115.txt\n",
      "Saved: 116.txt\n",
      "Saved: 117.txt\n",
      "Saved: 118.txt\n",
      "Saved: 119.txt\n",
      "Saved: 120.txt\n",
      "Saved: 121.txt\n",
      "Saved: 122.txt\n",
      "Saved: 123.txt\n",
      "Saved: 124.txt\n",
      "Saved: 125.txt\n",
      "Saved: 126.txt\n",
      "Saved: 127.txt\n",
      "Saved: 128.txt\n",
      "Saved: 129.txt\n",
      "Saved: 130.txt\n",
      "Saved: 131.txt\n",
      "Saved: 132.txt\n",
      "Saved: 133.txt\n",
      "Saved: 134.txt\n",
      "Saved: 135.txt\n",
      "Saved: 136.txt\n",
      "Saved: 137.txt\n",
      "Saved: 138.txt\n",
      "Saved: 139.txt\n",
      "Saved: 140.txt\n",
      "Saved: 141.txt\n",
      "Saved: 142.txt\n",
      "Saved: 143.txt\n",
      "Saved: 144.txt\n",
      "Saved: 145.txt\n",
      "Saved: 146.txt\n",
      "Saved: 147.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Load your Excel file first\n",
    "# df = pd.read_excel(\"your_excel_file.xlsx\")  # Make sure this is loaded before running the loop\n",
    "\n",
    "url_id = 1\n",
    "\n",
    "for i in range(len(df)):\n",
    "    try:\n",
    "        url = df.iloc[i, 0]  # Assuming the URL is in the first column\n",
    "\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'\n",
    "        }\n",
    "\n",
    "        # Fetch the URL content\n",
    "        page = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        # Extract article content\n",
    "        content = soup.findAll(attrs={'class': 'td-post-content'})\n",
    "        if not content:\n",
    "            print(f\"Content not found for URL: {url}\")\n",
    "            continue\n",
    "\n",
    "        content_text = content[0].text.replace('\\xa0', \" \").replace('\\n', \" \")\n",
    "\n",
    "        # Extract title\n",
    "        title = soup.findAll(attrs={'class': 'entry-title'})\n",
    "        if not title or len(title) < 1:\n",
    "            print(f\"Title not found for URL: {url}\")\n",
    "            continue\n",
    "\n",
    "        title_text = title[0].text.replace('\\n', \" \").replace('/', \"\")\n",
    "\n",
    "        # Combine title and content\n",
    "        full_text = title_text + \". \" + content_text\n",
    "\n",
    "        # Save to text file\n",
    "        file_name = f\"{url_id}.txt\"\n",
    "        with open(file_name, 'w', encoding='utf-8') as f:\n",
    "            f.write(full_text)\n",
    "\n",
    "        print(f\"Saved: {file_name}\")\n",
    "        url_id += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error at URL {url_id}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7994a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#importing each extracted files\n",
    "import os\n",
    "import pandas as pd \n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# List all files in the current directory\n",
    "files = os.listdir(current_directory)\n",
    "\n",
    "# Filter for .txt files\n",
    "txt_files = [file for file in files if file.endswith('.txt')]\n",
    "\n",
    "# Initialize an empty list to store the dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Loop through each .txt file and read it into a dataframe\n",
    "for file in txt_files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        df = pd.DataFrame({'content': [content]})\n",
    "        dataframes.append(df)\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Save the combined dataframe to a CSV file\n",
    "combined_df.to_csv('combined_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "text_analysis_header",
   "metadata": {},
   "source": [
    "# Text Analysis Phase\n",
    "## Comprehensive Text Analytics including Sentiment Analysis, Readability Metrics, and Text Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import_text_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional libraries for text analysis\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import syllables\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_dictionaries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load positive and negative word dictionaries\n",
    "# You can create these files or use existing sentiment lexicons\n",
    "\n",
    "# Create basic positive and negative word lists\n",
    "positive_words = [\n",
    "    'good', 'excellent', 'amazing', 'wonderful', 'fantastic', 'great', 'awesome', \n",
    "    'outstanding', 'brilliant', 'superb', 'magnificent', 'marvelous', 'terrific',\n",
    "    'perfect', 'beautiful', 'love', 'like', 'enjoy', 'happy', 'pleased', 'satisfied',\n",
    "    'delighted', 'thrilled', 'excited', 'positive', 'successful', 'effective',\n",
    "    'efficient', 'innovative', 'creative', 'impressive', 'remarkable', 'exceptional'\n",
    "]\n",
    "\n",
    "negative_words = [\n",
    "    'bad', 'terrible', 'awful', 'horrible', 'disgusting', 'hate', 'dislike',\n",
    "    'poor', 'worst', 'disappointing', 'frustrating', 'annoying', 'boring',\n",
    "    'sad', 'angry', 'upset', 'worried', 'concerned', 'problem', 'issue',\n",
    "    'difficulty', 'challenge', 'failure', 'error', 'mistake', 'wrong',\n",
    "    'negative', 'ineffective', 'inefficient', 'useless', 'worthless'\n",
    "]\n",
    "\n",
    "# Convert to sets for faster lookup\n",
    "positive_words_set = set(positive_words)\n",
    "negative_words_set = set(negative_words)\n",
    "\n",
    "print(f\"Loaded {len(positive_words)} positive words and {len(negative_words)} negative words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "text_analysis_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.personal_pronouns = {\n",
    "            'i', 'we', 'my', 'ours', 'us', 'me', 'our', 'you', 'your', 'yours'\n",
    "        }\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and preprocess text\"\"\"\n",
    "        # Remove special characters and digits, keep only alphabets and spaces\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove extra whitespaces\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    \n",
    "    def count_syllables(self, word):\n",
    "        \"\"\"Count syllables in a word\"\"\"\n",
    "        try:\n",
    "            return syllables.estimate(word)\n",
    "        except:\n",
    "            # Fallback method\n",
    "            word = word.lower()\n",
    "            vowels = 'aeiouy'\n",
    "            syllable_count = 0\n",
    "            previous_was_vowel = False\n",
    "            \n",
    "            for char in word:\n",
    "                is_vowel = char in vowels\n",
    "                if is_vowel and not previous_was_vowel:\n",
    "                    syllable_count += 1\n",
    "                previous_was_vowel = is_vowel\n",
    "            \n",
    "            # Handle silent 'e'\n",
    "            if word.endswith('e'):\n",
    "                syllable_count -= 1\n",
    "            \n",
    "            # Every word has at least one syllable\n",
    "            return max(1, syllable_count)\n",
    "    \n",
    "    def is_complex_word(self, word):\n",
    "        \"\"\"Check if word is complex (more than 2 syllables)\"\"\"\n",
    "        return self.count_syllables(word) > 2\n",
    "    \n",
    "    def sentiment_analysis(self, text):\n",
    "        \"\"\"Perform sentiment analysis\"\"\"\n",
    "        # Clean text for analysis\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        words = word_tokenize(cleaned_text)\n",
    "        \n",
    "        # Count positive and negative words\n",
    "        positive_score = sum(1 for word in words if word in positive_words_set)\n",
    "        negative_score = sum(1 for word in words if word in negative_words_set)\n",
    "        \n",
    "        # Calculate polarity score\n",
    "        polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "        \n",
    "        # Calculate subjectivity score using TextBlob\n",
    "        blob = TextBlob(text)\n",
    "        subjectivity_score = blob.sentiment.subjectivity\n",
    "        \n",
    "        return {\n",
    "            'positive_score': positive_score,\n",
    "            'negative_score': negative_score,\n",
    "            'polarity_score': polarity_score,\n",
    "            'subjectivity_score': subjectivity_score\n",
    "        }\n",
    "    \n",
    "    def readability_analysis(self, text):\n",
    "        \"\"\"Calculate readability metrics\"\"\"\n",
    "        # Tokenize sentences and words\n",
    "        sentences = sent_tokenize(text)\n",
    "        words = word_tokenize(self.clean_text(text))\n",
    "        \n",
    "        # Filter out stop words and empty strings\n",
    "        words = [word for word in words if word and word not in self.stop_words]\n",
    "        \n",
    "        # Basic counts\n",
    "        word_count = len(words)\n",
    "        sentence_count = len(sentences)\n",
    "        \n",
    "        # Calculate syllables and complex words\n",
    "        total_syllables = sum(self.count_syllables(word) for word in words)\n",
    "        complex_words = [word for word in words if self.is_complex_word(word)]\n",
    "        complex_word_count = len(complex_words)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0\n",
    "        percentage_complex_words = (complex_word_count / word_count * 100) if word_count > 0 else 0\n",
    "        fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "        \n",
    "        # Average words per sentence\n",
    "        avg_words_per_sentence = word_count / sentence_count if sentence_count > 0 else 0\n",
    "        \n",
    "        # Count personal pronouns\n",
    "        personal_pronoun_count = sum(1 for word in word_tokenize(text.lower()) \n",
    "                                   if word in self.personal_pronouns)\n",
    "        \n",
    "        # Average word length\n",
    "        avg_word_length = sum(len(word) for word in words) / word_count if word_count > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'word_count': word_count,\n",
    "            'sentence_count': sentence_count,\n",
    "            'avg_sentence_length': avg_sentence_length,\n",
    "            'percentage_complex_words': percentage_complex_words,\n",
    "            'fog_index': fog_index,\n",
    "            'avg_words_per_sentence': avg_words_per_sentence,\n",
    "            'complex_word_count': complex_word_count,\n",
    "            'syllable_count': total_syllables,\n",
    "            'personal_pronoun_count': personal_pronoun_count,\n",
    "            'avg_word_length': avg_word_length\n",
    "        }\n",
    "    \n",
    "    def analyze_text(self, text):\n",
    "        \"\"\"Perform comprehensive text analysis\"\"\"\n",
    "        sentiment_results = self.sentiment_analysis(text)\n",
    "        readability_results = self.readability_analysis(text)\n",
    "        \n",
    "        # Combine all results\n",
    "        results = {**sentiment_results, **readability_results}\n",
    "        return results\n",
    "\n",
    "# Initialize the analyzer\n",
    "analyzer = TextAnalyzer()\n",
    "print(\"Text Analyzer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze_all_texts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the combined data\n",
    "df_combined = pd.read_csv('combined_data.csv')\n",
    "print(f\"Loaded {len(df_combined)} text documents for analysis\")\n",
    "\n",
    "# Analyze each text document\n",
    "analysis_results = []\n",
    "\n",
    "for idx, row in df_combined.iterrows():\n",
    "    text = row['content']\n",
    "    \n",
    "    # Perform text analysis\n",
    "    results = analyzer.analyze_text(text)\n",
    "    \n",
    "    # Add document ID\n",
    "    results['document_id'] = idx + 1\n",
    "    \n",
    "    analysis_results.append(results)\n",
    "    \n",
    "    if (idx + 1) % 10 == 0:\n",
    "        print(f\"Analyzed {idx + 1} documents...\")\n",
    "\n",
    "print(f\"\\nCompleted analysis of all {len(analysis_results)} documents!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_results_dataframe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with analysis results\n",
    "results_df = pd.DataFrame(analysis_results)\n",
    "\n",
    "# Reorder columns for better readability\n",
    "column_order = [\n",
    "    'document_id',\n",
    "    'positive_score',\n",
    "    'negative_score', \n",
    "    'polarity_score',\n",
    "    'subjectivity_score',\n",
    "    'word_count',\n",
    "    'sentence_count',\n",
    "    'avg_sentence_length',\n",
    "    'percentage_complex_words',\n",
    "    'fog_index',\n",
    "    'avg_words_per_sentence',\n",
    "    'complex_word_count',\n",
    "    'syllable_count',\n",
    "    'personal_pronoun_count',\n",
    "    'avg_word_length'\n",
    "]\n",
    "\n",
    "results_df = results_df[column_order]\n",
    "\n",
    "# Display first few rows\n",
    "print(\"Text Analysis Results:\")\n",
    "print(\"=\" * 50)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "results_df.to_csv('text_analysis_results.csv', index=False)\n",
    "print(\"Results saved to 'text_analysis_results.csv'\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "print(results_df.describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Text Analysis Results Visualization', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Sentiment Distribution\n",
    "axes[0, 0].hist(results_df['polarity_score'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Distribution of Polarity Scores')\n",
    "axes[0, 0].set_xlabel('Polarity Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].axvline(x=0, color='red', linestyle='--', alpha=0.7, label='Neutral')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Subjectivity Distribution\n",
    "axes[0, 1].hist(results_df['subjectivity_score'], bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[0, 1].set_title('Distribution of Subjectivity Scores')\n",
    "axes[0, 1].set_xlabel('Subjectivity Score')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# 3. Word Count Distribution\n",
    "axes[0, 2].hist(results_df['word_count'], bins=20, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[0, 2].set_title('Distribution of Word Counts')\n",
    "axes[0, 2].set_xlabel('Word Count')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "\n",
    "# 4. Fog Index Distribution\n",
    "axes[1, 0].hist(results_df['fog_index'], bins=20, alpha=0.7, color='purple', edgecolor='black')\n",
    "axes[1, 0].set_title('Distribution of Fog Index (Readability)')\n",
    "axes[1, 0].set_xlabel('Fog Index')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# 5. Scatter plot: Polarity vs Subjectivity\n",
    "axes[1, 1].scatter(results_df['polarity_score'], results_df['subjectivity_score'], \n",
    "                   alpha=0.6, color='red', s=30)\n",
    "axes[1, 1].set_title('Polarity vs Subjectivity')\n",
    "axes[1, 1].set_xlabel('Polarity Score')\n",
    "axes[1, 1].set_ylabel('Subjectivity Score')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Complex Words Percentage\n",
    "axes[1, 2].hist(results_df['percentage_complex_words'], bins=20, alpha=0.7, color='brown', edgecolor='black')\n",
    "axes[1, 2].set_title('Distribution of Complex Words %')\n",
    "axes[1, 2].set_xlabel('Percentage of Complex Words')\n",
    "axes[1, 2].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('text_analysis_visualization.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nVisualization saved as 'text_analysis_visualization.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis and insights\n",
    "print(\"DETAILED TEXT ANALYSIS INSIGHTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sentiment Analysis Insights\n",
    "positive_docs = len(results_df[results_df['polarity_score'] > 0])\n",
    "negative_docs = len(results_df[results_df['polarity_score'] < 0])\n",
    "neutral_docs = len(results_df[results_df['polarity_score'] == 0])\n",
    "\n",
    "print(f\"\\n📊 SENTIMENT ANALYSIS:\")\n",
    "print(f\"   • Positive documents: {positive_docs} ({positive_docs/len(results_df)*100:.1f}%)\")\n",
    "print(f\"   • Negative documents: {negative_docs} ({negative_docs/len(results_df)*100:.1f}%)\")\n",
    "print(f\"   • Neutral documents: {neutral_docs} ({neutral_docs/len(results_df)*100:.1f}%)\")\n",
    "print(f\"   • Average polarity: {results_df['polarity_score'].mean():.3f}\")\n",
    "print(f\"   • Average subjectivity: {results_df['subjectivity_score'].mean():.3f}\")\n",
    "\n",
    "# Readability Insights\n",
    "print(f\"\\n📖 READABILITY ANALYSIS:\")\n",
    "print(f\"   • Average word count: {results_df['word_count'].mean():.1f}\")\n",
    "print(f\"   • Average sentence count: {results_df['sentence_count'].mean():.1f}\")\n",
    "print(f\"   • Average sentence length: {results_df['avg_sentence_length'].mean():.1f} words\")\n",
    "print(f\"   • Average Fog Index: {results_df['fog_index'].mean():.1f}\")\n",
    "print(f\"   • Average complex words: {results_df['percentage_complex_words'].mean():.1f}%\")\n",
    "\n",
    "# Document complexity classification\n",
    "easy_docs = len(results_df[results_df['fog_index'] < 12])\n",
    "medium_docs = len(results_df[(results_df['fog_index'] >= 12) & (results_df['fog_index'] < 16)])\n",
    "hard_docs = len(results_df[results_df['fog_index'] >= 16])\n",
    "\n",
    "print(f\"\\n📚 DOCUMENT COMPLEXITY:\")\n",
    "print(f\"   • Easy to read (Fog < 12): {easy_docs} ({easy_docs/len(results_df)*100:.1f}%)\")\n",
    "print(f\"   • Medium difficulty (12 ≤ Fog < 16): {medium_docs} ({medium_docs/len(results_df)*100:.1f}%)\")\n",
    "print(f\"   • Hard to read (Fog ≥ 16): {hard_docs} ({hard_docs/len(results_df)*100:.1f}%)\")\n",
    "\n",
    "# Top insights\n",
    "most_positive = results_df.loc[results_df['polarity_score'].idxmax()]\n",
    "most_negative = results_df.loc[results_df['polarity_score'].idxmin()]\n",
    "most_complex = results_df.loc[results_df['fog_index'].idxmax()]\n",
    "longest_doc = results_df.loc[results_df['word_count'].idxmax()]\n",
    "\n",
    "print(f\"\\n🏆 TOP INSIGHTS:\")\n",
    "print(f\"   • Most positive document: #{most_positive['document_id']} (score: {most_positive['polarity_score']:.3f})\")\n",
    "print(f\"   • Most negative document: #{most_negative['document_id']} (score: {most_negative['polarity_score']:.3f})\")\n",
    "print(f\"   • Most complex document: #{most_complex['document_id']} (Fog Index: {most_complex['fog_index']:.1f})\")\n",
    "print(f\"   • Longest document: #{longest_doc['document_id']} ({longest_doc['word_count']} words)\")\n",
    "\n",
    "print(f\"\\n✅ Analysis completed successfully!\")\n",
    "print(f\"📁 Output files created:\")\n",
    "print(f\"   • text_analysis_results.csv\")\n",
    "print(f\"   • text_analysis_visualization.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
